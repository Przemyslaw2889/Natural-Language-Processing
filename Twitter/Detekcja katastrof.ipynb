{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/przemyslaw/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: tensorboard<1.9.0,>=1.8.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: setuptools in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Requirement already satisfied: bleach==1.5.0 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /home/przemyslaw/anaconda3/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detekcja katastrof na podstawie tweetów z Twittera\n",
    "Dlaczego analizowanie tekstu z mediów społecznościowych może być dla nas użyteczne? Bo znajduje się tam mnóstwo informacji o emocjach i opiniach ludzi. W wielu sytuacjach odpowiedzi na interesujęace nas pytania znajdują się w słowach ludzi. Przykładowo jakaś firma mogłaby chcieć bez wkładania wysiłku w badania poznać opinie o swoim produkcie - czy ludziom się podoba, na co narzekają, czy u jakiejś większej grupy zaczynają pojawiać się usterki spowodowane być może błędem w produkcji itp. \n",
    "\n",
    "Problemem jest jednak fakt, że tekst trudno jest nam reprezentować. Trudno zakodować go w postaci, z której będzie można odczytać jego prawdziwe znaczenie. Z pomocą przychodzą nam na szczęście metody przetwarzania języka naturalnego.\n",
    "\n",
    "W tym notebooku postaram się w automatyczny sposób sprawdzać czy dany tweet zawiera informację o katastrofie. Czasem zdarza się, że analiza takich danych może w szybszy sposób zauważyć na przykład trzęsienia ziemi niż naukowcy wyposażeni w odpowiednie przyrządy (trzęsięnie ziemi w Sichuan, Chiny, 2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dane\n",
    "Nasze dane są w postaci tsv (tab separated values). Zawierają trzy kolumny - tekst, kolumnę choose_one, która zawiera informaję czy tweet jest o jakiejś katastrofie i class_label - 0 lub 1. Istnieje też opcja, że osoba która klasyfikowała ręcznie te dane nie była w stanie stwierdzić do jakiej kategorii należy tweet. Wtedy choose_one jest 'Can't Decide', a class_label wynosi 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text choose_one  class_label\n",
       "0                 Just happened a terrible car crash   Relevant            1\n",
       "1  Our Deeds are the Reason of this #earthquake M...   Relevant            1\n",
       "2  Heard about #earthquake is different cities, s...   Relevant            1\n",
       "3  there is a forest fire at spot pond, geese are...   Relevant            1\n",
       "4             Forest fire near La Ronge Sask. Canada   Relevant            1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "disasters_dataset = pd.read_table('../notebooks/data/disasters_socialmedia.tsv')\n",
    "disasters_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W naszym zbiorze danych jest trochę więcej danych z kategorii 'tweety zwyczajne' niż 'tweety katastrofalne'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_label\n",
       "0    6187\n",
       "1    4673\n",
       "2      16\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disasters_dataset.groupby('class_label')['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Relevant', 'Not Relevant', \"Can't Decide\"], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disasters_dataset.choose_one.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej jest lista tweetów, w których osoby nie mogły zdecydować się do jakiej kategorii dany tweet należy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Why is there an ambulance right outside my work\n",
      "#1 @MisfitRarity misfit got bombed\n",
      "#2 @RockBottomRadFM Is one of the challenges on Tough Enough rescuing people from burning buildings?\n",
      "#3 ? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying\n",
      "#4 What if we used drones to help firefighters lead people out of burning buildings/ help put the fire out?\n",
      "#5 San Bernardino I10 W Eo / Redlands Blvd **Trfc Collision-No Inj** http://t.co/FT9KIGmIgh\n",
      "#6 Kinetic Typography Crash Course (After Effects) (Video) http://t.co/fL8gCi84Aj #course http://t.co/dVONWIv3l1\n",
      "#7 Deaths 5 http://t.co/0RtxTT11jj\n",
      "#8 @MythGriy they can't detonate unless they touch the ground\n",
      "#9 MPD director Armstrong: when this first happened I cannot begin to tell you the devastation I felt.\n",
      "#10 Displaced Persons GN (2014 Image) #1-1ST NM http://t.co/yEJt18sbm0 http://t.co/RcqacN91bE\n",
      "#11 Large rain drops falling in Rock Hill off Anderson Road. #rain #scwx #drought\n",
      "#12 I See Fire\n",
      "#13 it's actually funny how chihaya remains a relatively static character for most of series so far and then suddenly develops like a landslide\n",
      "#14 this storm????\n",
      "#15 @FoxNews my father's mother survived it. Never met her.\n"
     ]
    }
   ],
   "source": [
    "for i,text in enumerate(disasters_dataset[disasters_dataset['class_label']==2]['text']): print('#{}'.format(i), text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "disasters_dataset = disasters_dataset.drop('choose_one', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of casual tweets: 0.570\n",
      "\n",
      "==================\n",
      "DISASTEROUS TWEETS\n",
      "==================\n",
      "------\n",
      " NetNewsLedger Wild Fire Update  - August 4 2015: THUNDER BAY  - WEATHER  - Thereξwere no new fires confirmed by t... http://t.co/JflxgEmBdA\n",
      "------\n",
      " @freddiedeboer @Thucydiplease then you have rise of Coates Charleston massacre Walter Scott and black twitter more broadly as well.\n",
      "------\n",
      " The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/dOFRh5YB01\n",
      "------\n",
      " CHS issues Hazardous Weather Outlook (HWO)  http://t.co/lbkiyfwFlU #WX\n",
      "------\n",
      " Earthquake : M 3.4 - 96km N of Brenas Puerto Rico: Time2015-08-05 10:34:24 UTC2015-08-05 06:34:24 -04:00 at ﾗ_ http://t.co/sDZrrfZhMy\n",
      "\n",
      "==================\n",
      "CASUAL TWEETS\n",
      "==================\n",
      "------\n",
      " Greece's tax revenues collapse as debt crisis continues: As talks continue over proposed  �86bn third bailout ... http://t.co/7w2WiEFjuq\n",
      "------\n",
      " #yyc #hailstorm #christmas came early https://t.co/f0A2IIzx3A\n",
      "------\n",
      " @StevenOnTwatter @PussyxDestroyer just order a blizzard pay then put your nuts in it say they have you ball flavored. Boom free ice cream\n",
      "------\n",
      " http://t.co/JwIv6WYW6F Osage Beach releases name\n",
      "------\n",
      " @FAIRx818x @PlayOverwatch @BlizzardCS please blizzard we love you\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "disasterous = disasters_dataset[disasters_dataset['class_label']==1]\n",
    "casual = disasters_dataset[disasters_dataset['class_label']==0]\n",
    "\n",
    "print('Fraction of casual tweets: {:.3f}'.format(len(casual)/(len(disasterous)+len(casual))))\n",
    "print('\\n==================\\nDISASTEROUS TWEETS\\n==================')\n",
    "for tweet in random.sample(list(disasterous.text), 5):\n",
    "    print('------\\n',tweet)\n",
    "print('\\n==================\\nCASUAL TWEETS\\n==================')\n",
    "for tweet in random.sample(list(casual.text), 5):\n",
    "    print('------\\n',tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dalszej części będę korzystał z biblioteki keras\n",
    "(https://keras.io/#you-have-just-found-keras)\n",
    "\n",
    "https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprezentacja tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=' ',\n",
    "                      char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(disasters_dataset.text)\n",
    "\n",
    "EncodedDataset = namedtuple('EncodedDataset', ('instances', 'labels'))\n",
    "\n",
    "texts_enco = tokenizer.texts_to_sequences(disasters_dataset.text)\n",
    "texts_padded = sequence.pad_sequences(texts_enco, padding='pre')\n",
    "encoded_dataset = EncodedDataset(instances=texts_padded, labels=disasters_dataset['class_label'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequence_len = len(texts_padded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_dataset.instances[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(x) for x in texts_enco])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To co zrobiliśmy to zamieniliśmy tworzyliśmy tokenizer, który uczy się na naszym korpusie tweetów. Wywołując metodę fit_on_texts tokenizer zapamiętał wszystkie słowa jakie kiedykolwiek zostały użyte (zbiór tych słów będę nazywał słownikiem). Następnie w texts_enco umieściliśmy poszczególne tweety jako ciągi wektorów długości tweeta (w słowach) w którym na k-tym miejscu jest numer k-tego słowa w słowniku. Tweety na szczęście są niedługie (najdłuższy ma długość 33), więc możemy sprawić, że każdy wektor będzie miał długość 33. Nową reprezentację tweetów umieszczamy w texts_padded (jeżeli tweet jest krótszy niż 33 słowa to na poprzednich miejscach ma zera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0   33\n",
      "  796    5 1520  132   98]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0  115 5930   24    4  858    8   21  254\n",
      "  153 1823 3839   89   42]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  384   55  254   11\n",
      " 1320 1824  660 1521  275]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   78   11    5  170   44   20  834 2545 8782   24 4613  902    4  691\n",
      "   10 1383  495  101   42]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0  170   44\n",
      "  214  903 8783 8784 1447]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0   42 1609 1522\n",
      "    6 8785    7 5931   24  144 8786   19 1825   40  283  279   58 2282\n",
      "    7  797 1711   24 1269]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  772 2836   62 4614\n",
      " 1384  279 1711    7  112]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0   33  103 1523   21  293   23 5932 2283   31  284   23\n",
      " 1384 8787   71    5  210]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 3261  418  112 1964  859  904    7  820 8788  419    6\n",
      " 1270  420   44 5933 1384]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  392 3262 5934 1384]\n"
     ]
    }
   ],
   "source": [
    "for instance in encoded_dataset.instances[:10]:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: ta metoda ma oczywiście swoje wady. Nie bierzemy pod uwagę w żaden sposób kolejności występowania słów i nie nadajemy im żadnego znaczenia. Można jednak oczekiwać, że będziemy w stanie z pewnym prawdopodobieństwem wykrywać tweety o katastrofach na podtawie samego tylko występowania poszczególnych słów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podział na zbiór treningowy i testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 8157, Validation data size: 2719\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "def get_splits(dataset, valid_fraction=0.25):\n",
    "    data_size = len(dataset.instances)\n",
    "    split_id = int(data_size * valid_fraction)\n",
    "    merged_data = list(zip(dataset.instances, dataset.labels))\n",
    "    random.shuffle(merged_data)\n",
    "    shuffled_instances, shuffled_labels = zip(*merged_data)\n",
    "    valid_data = EncodedDataset(instances=shuffled_instances[0:split_id],\n",
    "                                labels=shuffled_labels[0:split_id])\n",
    "    train_data = EncodedDataset(instances=shuffled_instances[split_id:],\n",
    "                                labels=shuffled_labels[split_id:])\n",
    "    return train_data, valid_data\n",
    "\n",
    "train_data, valid_data = get_splits(encoded_dataset)\n",
    "\n",
    "print('Training data size: {}, Validation data size: {}'.format(len(train_data.labels), len(valid_data.labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzięki kerasowi budowanie modelu jest bardzo proste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model... done\n",
      "Training model..\n",
      "Train on 8157 samples, validate on 2719 samples\n",
      "Epoch 1/5\n",
      "8157/8157 [==============================] - 17s 2ms/step - loss: 0.6711 - acc: 0.5961 - val_loss: 0.6589 - val_acc: 0.6278\n",
      "Epoch 2/5\n",
      "8157/8157 [==============================] - 15s 2ms/step - loss: 0.6488 - acc: 0.6300 - val_loss: 0.6466 - val_acc: 0.6289\n",
      "Epoch 3/5\n",
      "8157/8157 [==============================] - 16s 2ms/step - loss: 0.6370 - acc: 0.6431 - val_loss: 0.6329 - val_acc: 0.6488\n",
      "Epoch 4/5\n",
      "8157/8157 [==============================] - 14s 2ms/step - loss: 0.6203 - acc: 0.6686 - val_loss: 0.6244 - val_acc: 0.6587\n",
      "Epoch 5/5\n",
      "8157/8157 [==============================] - 14s 2ms/step - loss: 0.5971 - acc: 0.6889 - val_loss: 0.6214 - val_acc: 0.6679\n",
      "Training comleted\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Embedding\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print('Building model...', end=' ')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_length=sequence_len, input_dim=vocab_size, output_dim=50, mask_zero=True)) \n",
    "# Set mask_zero=False when using CNN\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
    "print('done')\n",
    "\n",
    "print('Training model..')\n",
    "x_train = np.array(train_data.instances, np.float32)\n",
    "y_train = np.array(train_data.labels, np.int32)\n",
    "x_valid = np.array(valid_data.instances, np.float32)\n",
    "y_valid = np.array(valid_data.labels, np.int32)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), batch_size=32, epochs=5, verbose=1)\n",
    "print('Training comleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation..\n",
      "2719/2719 [==============================] - 1s 441us/step\n",
      "Accuracy on validation set: 0.668\n"
     ]
    }
   ],
   "source": [
    "print(\"Final validation..\")\n",
    "_, acc = model.evaluate(x_valid, y_valid, verbose=1) \n",
    "print(\"Accuracy on validation set: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skuteczność na poziomie 0.67 jest lepsza niż całkiem losowa, ale nadal niezbyt satysfakcjonująca. Będzie trzeba spróbować ją poprawić eksperymentując z innymi modelami (być może bardziej skom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
